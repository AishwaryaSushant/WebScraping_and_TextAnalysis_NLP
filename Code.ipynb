{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPmIdztcTy08pDwPR0rLjEb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AishwaryaSushant/WebScraping_and_TextAnalysis_NLP/blob/main/Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code by Aishwarya Sushant\n"
      ],
      "metadata": {
        "id": "ro9XMY3bGDSs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gOOdVzqDcze",
        "outputId": "99cec22c-2a3c-40ed-fe43-b8a0d7659c57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Article not available\n",
            "Error: Article not available\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Load input data\n",
        "input_df = pd.read_excel('/content/Input.xlsx')\n",
        "\n",
        "# Initialize empty DataFrame for output\n",
        "output_df = pd.DataFrame(columns=['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'])\n",
        "\n",
        "# Create a new dictionary to store the data\n",
        "data_dict = {}\n",
        "\n",
        "# Initialize the NLTK tokenizer and download required resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to calculate the FOG Index\n",
        "def fog_index(avg_sentence_length, percentage_of_complex_words):\n",
        "    return 0.4 * (avg_sentence_length + percentage_of_complex_words)\n",
        "\n",
        "# Function to calculate the percentage of complex words\n",
        "def percentage_complex_words(complex_word_count, word_count):\n",
        "    return (complex_word_count / word_count) * 100\n",
        "\n",
        "# Function to count syllables in a word\n",
        "def count_syllables(word):\n",
        "    vowels = \"aeiouAEIOU\"\n",
        "    count = 0\n",
        "    prev_char = \"\"\n",
        "    for char in word:\n",
        "        if char in vowels and prev_char not in vowels:\n",
        "            count += 1\n",
        "        prev_char = char\n",
        "    if word.endswith(('es', 'ed')):\n",
        "        count -= 1\n",
        "    return max(count, 1)\n",
        "\n",
        "# Read positive and negative words from text files\n",
        "def read_words_from_file(file_path):\n",
        "    words = []\n",
        "    with open(file_path, encoding='ISO-8859-1') as f:\n",
        "        for line in f:\n",
        "            word = line.strip()\n",
        "            words.append(word)\n",
        "    return words\n",
        "\n",
        "# Open the .txt files containing stopwords\n",
        "stopword_paths = ['/content/StopWords_Auditor.txt', '/content/StopWords_Currencies.txt', '/content/StopWords_DatesandNumbers.txt', '/content/StopWords_Generic.txt', '/content/StopWords_GenericLong.txt', '/content/StopWords_Names.txt']\n",
        "\n",
        "# Read stopwords from the .txt files and create a set for faster lookup\n",
        "SW = set()\n",
        "for stopword_path in stopword_paths:\n",
        "    SW.update(read_words_from_file(stopword_path))\n",
        "\n",
        "# Initialize the Master_dictionary\n",
        "Master_dictionary = {}\n",
        "\n",
        "# Open the .txt files containing positive and negative words\n",
        "positive_words_path = '/content/positive-words.txt'\n",
        "negative_words_path = '/content/negative-words.txt'\n",
        "\n",
        "# Read positive and negative words from the .txt files\n",
        "positive_words = read_words_from_file(positive_words_path)\n",
        "negative_words = read_words_from_file(negative_words_path)\n",
        "\n",
        "# Add positive words to Master_dictionary if they are not in the stopwords list\n",
        "for word in positive_words:\n",
        "    if word not in SW:\n",
        "        Master_dictionary[word] = 'positive'\n",
        "\n",
        "# Add negative words to Master_dictionary if they are not in the stopwords list\n",
        "for word in negative_words:\n",
        "    if word not in SW:\n",
        "        Master_dictionary[word] = 'negative'\n",
        "\n",
        "# Function to calculate personal pronouns\n",
        "def count_personal_pronouns(text):\n",
        "    personal_pronouns = ['I', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']\n",
        "    count = 0\n",
        "    for pronoun in personal_pronouns:\n",
        "        count += len(re.findall(r'\\b' + re.escape(pronoun) + r'\\b', text, re.IGNORECASE))\n",
        "    return count\n",
        "\n",
        "# Loop through each URL in input data\n",
        "for index, row in input_df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    html = response.text\n",
        "\n",
        "    # Parse HTML with BeautifulSoup and extract the article text\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    article_element = soup.find('article')  # Locate the <article> element\n",
        "\n",
        "    title = []\n",
        "    para = []\n",
        "\n",
        "    # Check if the article element is present\n",
        "    if article_element is not None:\n",
        "        # Extract the title of the web page\n",
        "        title = soup.title.text\n",
        "\n",
        "        # Extract the text from the paragraph elements\n",
        "        para1 = soup.find_all(class_='td-post-content tagdiv-type')\n",
        "        para2 = soup.find_all(class_='tdb-block-inner td-fix-index')\n",
        "\n",
        "        para1_text = []\n",
        "        for paragraph in para1:\n",
        "            para1_text.append(paragraph.text)\n",
        "\n",
        "        para2_text = []\n",
        "        for paragraph2 in para2:\n",
        "            para2_text.append(paragraph2.text)\n",
        "\n",
        "        para = para1_text + para2_text\n",
        "\n",
        "        # Tokenize the text using NLTK\n",
        "        tokens = word_tokenize(' '.join(para))\n",
        "\n",
        "        # Calculate Positive Score, Negative Score, and other variables here\n",
        "        # Initialize scores and other variables\n",
        "        positive_score = 0\n",
        "        negative_score = 0\n",
        "        total_words = len(tokens)\n",
        "        total_sentences = len(sent_tokenize(' '.join(para)))\n",
        "\n",
        "        complex_word_count = 0\n",
        "        personal_pronouns = 0\n",
        "        total_syllables = 0\n",
        "\n",
        "        # Calculate Positive and Negative Scores based on Master_dictionary\n",
        "        for token in tokens:\n",
        "            if token in Master_dictionary:\n",
        "                if Master_dictionary[token] == 'positive':\n",
        "                    positive_score += 1\n",
        "                elif Master_dictionary[token] == 'negative':\n",
        "                    negative_score += 1\n",
        "\n",
        "        # Calculate Polarity Score and Subjectivity Score\n",
        "        polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
        "        subjectivity_score = (positive_score + negative_score) / (total_words + 0.000001)\n",
        "\n",
        "        # Calculate AVG SENTENCE LENGTH\n",
        "        avg_sentence_length = total_words / total_sentences\n",
        "\n",
        "        # Calculate PERCENTAGE OF COMPLEX WORDS\n",
        "        for token in tokens:\n",
        "            if token.lower() not in nltk.corpus.words.words() and token not in string.punctuation:\n",
        "                complex_word_count += 1\n",
        "                if token.lower() in ['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']:\n",
        "                    personal_pronouns += 1\n",
        "\n",
        "                # Calculate SYLLABLE PER WORD\n",
        "                syllables = count_syllables(token)\n",
        "                total_syllables += syllables\n",
        "                break\n",
        "\n",
        "        # Calculate FOG INDEX\n",
        "        fog = fog_index(avg_sentence_length, percentage_complex_words(complex_word_count, total_words))\n",
        "\n",
        "        # Calculate AVG NUMBER OF WORDS PER SENTENCE\n",
        "        avg_words_per_sentence = total_words / total_sentences\n",
        "\n",
        "        # Calculate AVG WORD LENGTH\n",
        "        avg_word_length = total_syllables / total_words\n",
        "\n",
        "        # Append the url_id, url, title, and paragraph text to the dictionary\n",
        "        data_dict[url_id] = {\n",
        "            'url': url,\n",
        "            'title': title,\n",
        "            'article_text': para,\n",
        "            'POSITIVE SCORE': positive_score,\n",
        "            'NEGATIVE SCORE': negative_score,\n",
        "            'POLARITY SCORE': polarity_score,\n",
        "            'SUBJECTIVITY SCORE': subjectivity_score,\n",
        "            'AVG SENTENCE LENGTH': avg_sentence_length,\n",
        "            'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words(complex_word_count, total_words),\n",
        "            'FOG INDEX': fog,\n",
        "            'AVG NUMBER OF WORDS PER SENTENCE': avg_words_per_sentence,\n",
        "            'COMPLEX WORD COUNT': complex_word_count,\n",
        "            'WORD COUNT': total_words,\n",
        "            'SYLLABLE PER WORD': avg_word_length,\n",
        "            'PERSONAL PRONOUNS': personal_pronouns,\n",
        "            'AVG WORD LENGTH': avg_word_length\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        # Print an error message if the article element is not present\n",
        "        print(\"Error: Article not available\")\n",
        "\n",
        "counter = 1\n",
        "print(counter)\n",
        "counter+1\n",
        "\n",
        "# Convert the data_dict to a DataFrame and save it\n",
        "output_df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
        "output_df.to_excel('/content/Output.xlsx', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjpTNKXSEFNP",
        "outputId": "49fc9cb8-6de7-4f43-c7ee-a7ccead00b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S5YXrU2CEP0P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}